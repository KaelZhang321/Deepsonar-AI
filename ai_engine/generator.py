"""
Chapter Generator Module

Handles the generation of individual chapters for long-form reports.
Uses CrewAI or direct LLM calls to generate chapter content with
structured reference output.
"""
import os
import asyncio
from typing import Dict, List, Tuple, Optional
from litellm import completion

from ai_engine.utils import parse_chapter_output, generate_chapter_prompt
from ai_engine.pre_search import pre_search, format_research_data


async def generate_single_chapter(
    topic: str, 
    chapter_info: Dict,
    previous_summary: str = "",
    search_count: int = 10,
    log_callback: Optional[callable] = None,
    report=None
) -> Tuple[str, List[Dict]]:
    """
    Generate a single chapter with research data and structured references.
    
    Args:
        topic: The main report topic
        chapter_info: Dict with 'title' and 'focus' keys
        previous_summary: Summary of previous chapters for context continuity
        search_count: Number of search results to fetch
        log_callback: Optional async callback for logging progress updates
        report: Optional Report instance to associate search results with
        
    Returns:
        Tuple of (chapter_content, list_of_references)
    """
    import chainlit as cl
    from ai_engine.pre_search import save_search_to_db
    
    chapter_title = chapter_info.get('title', 'ç« èŠ‚')
    chapter_focus = chapter_info.get('focus', '')
    
    # Helper function for logging
    async def log(msg: str):
        if log_callback:
            await log_callback(msg)
    
    # Construct search query from chapter context
    search_query = f"{topic} {chapter_focus}"
    
    await log(f"   ðŸ” æ­£åœ¨æœç´¢: {search_query[:50]}...")
    
    # Perform pre-search for this chapter
    search_data = await cl.make_async(lambda: pre_search(search_query, count=search_count))()
    
    # Log search results count
    result_count = len(search_data.get('raw_data', [])) if search_data else 0
    await log(f"   ðŸ“š æ‰¾åˆ° {result_count} æ¡ç›¸å…³èµ„æ–™")
    
    # Save search results to database with report association
    if search_data.get('raw_data') and report:
        try:
            await cl.make_async(lambda: save_search_to_db(search_query, search_data, report))()
            await log(f"   ðŸ’¾ æœç´¢ç»“æžœå·²ä¿å­˜å¹¶å…³è”åˆ°æŠ¥å‘Š")
        except Exception as e:
            await log(f"   âš ï¸ æœç´¢ç»“æžœä¿å­˜å¤±è´¥: {e}")
    
    research_context = format_research_data(search_query, search_data)
    
    await log(f"   âœï¸ AI æ­£åœ¨æ’°å†™ {chapter_title}...")
    
    # Build the generation prompt
    base_prompt = generate_chapter_prompt(topic, chapter_info, previous_summary)
    
    full_prompt = f"""
{base_prompt}

ã€æœç´¢èµ„æ–™ã€‘
ä»¥ä¸‹æ˜¯å…³äºŽæœ¬ç« ä¸»é¢˜çš„æœç´¢ç»“æžœï¼Œè¯·åŸºäºŽè¿™äº›çœŸå®žæ¥æºæ’°å†™å†…å®¹ã€‚
**é‡è¦ï¼šåœ¨æ­£æ–‡ä¸­ä½¿ç”¨ [Ref-1], [Ref-2] ç­‰æ ¼å¼å¼•ç”¨ï¼Œå¼•ç”¨ç¼–å·å¿…é¡»ä¸Žä¸‹æ–¹æ¥æºç¼–å·ä¸€ä¸€å¯¹åº”ã€‚**

{research_context}
"""
    
    # Call LLM to generate chapter
    try:
        await log(f"   ðŸ¤– è°ƒç”¨å¤§æ¨¡åž‹ç”Ÿæˆå†…å®¹...")
        
        response = await cl.make_async(lambda: completion(
            model="openai/" + os.getenv("ARK_MODEL_ENDPOINT", "ep-20250603140551-tp9lt"),
            messages=[{"role": "user", "content": full_prompt}],
            api_key=os.getenv("ARK_API_KEY"),
            base_url=os.getenv("ARK_BASE_URL", "https://ark.cn-beijing.volces.com/api/v3"),
            max_tokens=2000
        ))()
        
        raw_output = response.choices[0].message.content.strip()
        
        await log(f"   ðŸ“ å†…å®¹ç”Ÿæˆå®Œæˆï¼Œæ­£åœ¨è§£æž...")
        
        # Parse the output to extract content (ignore LLM's refs, use search data instead)
        content, _ = parse_chapter_output(raw_output)
        
        # CRITICAL: Use actual search data as references, not LLM-generated ones
        # This ensures all URLs are real and from the search results
        refs = []
        if search_data.get('raw_data'):
            for i, item in enumerate(search_data['raw_data'], 1):
                refs.append({
                    "id": f"[Ref-{i}]",
                    "url": item.get('url', ''),
                    "title": item.get('title', 'å‚è€ƒæ¥æº')
                })
        
        await log(f"   âœ… {chapter_title} æ’°å†™å®Œæˆ ({len(content)} å­—)")
        
        return content, refs
        
    except Exception as e:
        error_msg = f"ç« èŠ‚ç”Ÿæˆå¤±è´¥: {str(e)}"
        await log(f"   âŒ {error_msg}")
        return error_msg, []


async def generate_report_outline(topic: str) -> List[Dict]:
    """
    Generate a structured outline for the report.
    
    Args:
        topic: The report topic
        
    Returns:
        List of chapter info dicts with 'title' and 'focus' keys
    """
    import chainlit as cl
    
    outline_prompt = f"""
è¯·ä¸ºä»¥ä¸‹ä¸»é¢˜ç”Ÿæˆä¸€ä»½è¯¦ç»†çš„è¡Œä¸šåˆ†æžæŠ¥å‘Šå¤§çº²ï¼š

ä¸»é¢˜ï¼š{topic}

è¦æ±‚ï¼š
1. ç”Ÿæˆ 4-6 ä¸ªç« èŠ‚
2. æ¯ä¸ªç« èŠ‚æœ‰æ˜Žç¡®çš„æ ‡é¢˜å’Œç ”ç©¶é‡ç‚¹
3. ç« èŠ‚ä¹‹é—´é€»è¾‘é€’è¿›ï¼Œè¦†ç›–è¡Œä¸šåˆ†æžçš„æ ¸å¿ƒç»´åº¦
4. è¾“å‡º JSON æ•°ç»„æ ¼å¼

è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼éµå®ˆï¼‰ï¼š
[
  {{"title": "1. ç« èŠ‚æ ‡é¢˜", "focus": "æœ¬ç« ç ”ç©¶é‡ç‚¹å…³é”®è¯"}},
  {{"title": "2. ç« èŠ‚æ ‡é¢˜", "focus": "æœ¬ç« ç ”ç©¶é‡ç‚¹å…³é”®è¯"}},
  ...
]

åªè¾“å‡º JSON æ•°ç»„ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
    
    try:
        response = await cl.make_async(lambda: completion(
            model="openai/" + os.getenv("ARK_MODEL_ENDPOINT", "ep-20250603140551-tp9lt"),
            messages=[{"role": "user", "content": outline_prompt}],
            api_key=os.getenv("ARK_API_KEY"),
            base_url=os.getenv("ARK_BASE_URL", "https://ark.cn-beijing.volces.com/api/v3"),
            max_tokens=1000
        ))()
        
        raw_output = response.choices[0].message.content.strip()
        
        # Extract JSON from response
        import json
        import re
        
        # Try to find JSON array in the response
        json_match = re.search(r'\[[\s\S]*\]', raw_output)
        if json_match:
            outline = json.loads(json_match.group())
            return outline
        else:
            # Fallback to default outline
            return get_default_outline(topic)
            
    except Exception as e:
        print(f"Outline generation failed: {e}")
        return get_default_outline(topic)


def get_default_outline(topic: str) -> List[Dict]:
    """Return a default outline structure if LLM generation fails."""
    return [
        {"title": "1. è¡Œä¸šå®è§‚æ¦‚å†µ", "focus": f"{topic} å¸‚åœºè§„æ¨¡ å‘å±•åŽ†ç¨‹ æ”¿ç­–çŽ¯å¢ƒ"},
        {"title": "2. ç«žäº‰æ ¼å±€åˆ†æž", "focus": f"{topic} ä¸»è¦ä¼ä¸š å¸‚åœºä»½é¢ ç«žäº‰æ€åŠ¿"},
        {"title": "3. æŠ€æœ¯å‘å±•è¶‹åŠ¿", "focus": f"{topic} æ ¸å¿ƒæŠ€æœ¯ åˆ›æ–°çªç ´ æŠ€æœ¯ç“¶é¢ˆ"},
        {"title": "4. æ¶ˆè´¹è€…ä¸Žå¸‚åœºæ´žå¯Ÿ", "focus": f"{topic} ç”¨æˆ·ç”»åƒ æ¶ˆè´¹è¶‹åŠ¿ éœ€æ±‚ç—›ç‚¹"},
        {"title": "5. æœªæ¥å±•æœ›ä¸Žå»ºè®®", "focus": f"{topic} å‘å±•é¢„æµ‹ æŠ•èµ„æœºä¼š æˆ˜ç•¥å»ºè®®"}
    ]


def summarize_chapter(content: str, max_length: int = 200) -> str:
    """
    Create a brief summary of a chapter for context continuity.
    
    Args:
        content: The chapter content
        max_length: Maximum summary length
        
    Returns:
        Brief summary string
    """
    # Simple extraction: take the first paragraph or truncate
    lines = content.split('\n')
    summary_lines = []
    current_length = 0
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        if current_length + len(line) > max_length:
            break
        summary_lines.append(line)
        current_length += len(line)
    
    return ' '.join(summary_lines)[:max_length] + "..."
